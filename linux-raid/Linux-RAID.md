# Программный RAID на Linux
*В данной статье рассмотрено создание и последующая работа с программным RAID-массивом в операционной системе Linux. 
*
## Общие сведения о технологии RAID
RAID -- Redundant Array of Independent Disks -- избыточный массив независимых дисков -- технология виртуализации данных, при которой диски особым образом объединяют в массив, создавая избыточность данных, при этом повышается надежность сохранности данных. Существует несколько типов RAID-массивов:
1. RAID 0 (STRIPE) -- дисковый массив повышенной производительности, без отказоустойчивости.
В данном типе RAID выполняется объединение физических дисков в один "логический диск". При использовании  RAID 0 данные на диски записываются последовательно, это даёт скорость в работе с дисками из-за чередования чтения/записи. Отказ одного из дисков в данном типе RAID ведёт к потере данных на диске и полной неработоспособности RAID-массива. Минимальное количество дисков для создания массива -- 2.
2. RAID 1 (MIRROR) -- дисковый массив с дублированием (зеркалированием).
В данном типе RAID выполняется объединение дисков в "зеркало". При использовании RAID 1 одинаковые данные на диски записываются сразу на два диска, чтение с дисков может выполняться параллельно. В случае выхода из строя одного из дисков, выполняется его замена и дальнейшая синхронизация RAID-массива.
Минимальное количество дисков для создания массива -- 2.
3. RAID 3 -- отказоустойчивый массив с параллельной передачей данных и чётностью.
Данные записываются на первые два диска, на третьем диске выполняется запись контрольной суммы. В случае выхода из строя одного из дисков с данными, выполняется обратное вычисление битов информации (логическая операция XOR) из контрольной суммы. 
Минимальное количество дисков для создания массива -- 3.
4. RAID 5 -- отказоустойчивый массив, в котором информация диска чётности распределена между всеми дисками.
Минимальное количество дисков для создания массива -- 3.
5. RAID 6 -- отказоустойчивый массив с допустимым количеством одновременно вышедшим из строя дисков равным 2. Используется блоки данных диска чётности и блоки данных с кодом Рида-Соломона.
Минимальное количество дисков для создания массива -- 5.
5. RAID 1+0 -- отказоустойчивый массив, в котором формируется двухуровневая иерархия RAID 1 и RAID 0. Число дисков в данном типе массива всегда чётное (N=2*M, где M-количество зеркальных пар дисков). 
6. RAID 5+0 -- отказоустойчивый массив, комбинация RAID 5 и RAID 0 (STRIPE). Для реализации такого массива необходимо минимум 6 дисков.

*JBOD -- Just a Bunch of Disks -- конфигурация массива, при которой каждый из дисков доступен также, как если бы был подключен к обычному контроллеру.
Существуют и другие типы RAID-массивов (RAID 2, 2.0+ 4), некоторые из них яляются коммерческими технологиями [[2](https://www.youtube.com/watch?v=bHIU4rZj2y8)].

## Создание программного RAID 1 массива на рабочей системе
Имеем установленные диски: sda и sdb. Диск sda размечен разделами и на нём установлена операционная система.
```
# lsblk
sda      8:0    0    10G  0 disk
├─sda1   8:1    0   190M  0 part /boot
├─sda2   8:2    0   488M  0 part [SWAP]
└─sda3   8:3    0   9.3G  0 part /
sdb      8:16   0    10G  0 disk
```
Оба диска должны иметь абсолютно одинаковую ёмкость, либо допускается, что диск sdb будет большем ёмкости, чем sda. Проверим, что оба диска одинаковы:
```
# fdisk -l /dev/sda /dev/sdb

Disk /dev/sda: 10.7 GB, 10737418240 bytes
255 heads, 63 sectors/track, 1305 cylinders, total 20971520 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0x000aef2c

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048      391167      194560   83  Linux
/dev/sda2          391168     1390591      499712   82  Linux swap / Solaris
/dev/sda3         1390592    20969471     9789440   83  Linux

Disk /dev/sdb: 10.7 GB, 10737418240 bytes
255 heads, 63 sectors/track, 1305 cylinders, total 20971520 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
```
Значение ёмкости `10737418240 bytes` диска sda должно совпадать вплоть до байта или быть меньше, чем у диска sdb.
Далее копируем таблицу разделов с диска /dev/sda на диск /dev/sdb.
`# sfdisk -d /dev/sda | sfdisk --force /dev/sdb`
Далее собираем RAID-массив. Для создания и дальнейшей работы с RAID-массивом потребуется утилита mdadm. Данная утилина есть в стандартных репозитория операционной системы. Если утилита отсутствует в системе, то выполнил её установку.
1. В случае, если на сервере используется пакетный менеджер APT.
`sudo apt install mdadm`
2. В случае, если на сервере используется пакетный менеджер yum:
`sudo yum install mdadm`

У программы *mdadm* есть несколько режимов [mode] работы:
1. `-A, --assemble`
Assemble (сборка). Собрать компоненты ранее созданного массива в массив. Компоненты можно указывать явно, но можно и не указывать — тогда выполняется их поиск по суперблокам.
2. `-B, --build`
Build (построение). Собрать массив из компонентов, у которых нет суперблоков. Не выполняются никакие проверки, создание и сборка массива в принципе ничем не отличаются.
3. `-C, --create`
Create (создание). Создать новый массив на основе указанных устройств. Использовать суперблоки размещённые на каждом устройстве.
4. `-F, --follow, --monitor`
Monitor (наблюдение). Следить за изменением состояния устройств. Для RAID0 этот режим не имеет смысла.
5. `-G, --grow`
Grow (расширение или уменьшение). Расширение или уменьшение массива, включаются или удаляются новые диски.
6. `-I, --incremental`
Incremental Assembly (инкрементальная сборка). Добавление диска в массив.
7. `-M, --manage`
Manage (управление). Разнообразные операции по управлению массивом, такие как замена диска и пометка как сбойного.
8. Misc (разное). Действия, которые не относятся ни к одному из перечисленных выше режимов работы.
9. Auto-detect (автоообнаружение). Активация автоматически обнаруживаемых массивов в ядре Linux.

Формат вызова утилиты mdadm:
`mdadm [mode] [array] [options]`
Собираем RAID 1 массив (опция `--level=1`):
`# mdadm --create --verbose /dev/md0 --level=1 --raid-devices=2 /dev/sda1 /dev/sdb1`
Каждый из разделов дисков необходимо попарно добавить в RAID 1 массив.
`# mdadm --create --verbose /dev/md1 --level=1 --raid-devices=2 /dev/sda2 /dev/sdb2`
`# mdadm --create --verbose /dev/md2 --level=1 --raid-devices=2 /dev/sda3 /dev/sdb3`
и т.д.
Также данный процесс возможно выполнить одной командой
`mdadm --create /dev/md2 --level=1 --raid-devices=2 /dev/sd[a-b] --name="name_raid"`
После того, как система соберет массив, начнётся процесс синхронизации дисков. Чтобы посмотреть процесс синхронизации:
`cat /proc/mdstat`
Изначально система не запоминает какие RAID-массивы ей нужно создать и какие компоненты в них входят. После синронизации необходимо добавить информацию о собранном RAID-массиве в конфигурационный файл /etc/mdadm/mdadm.conf
Строки, которые необходимо добавить в конфигурационный файл RAID массива mdadm.conf можно получить при помощи команды:
` mdadm --detail --scan --verbose`
Если файла mdadm.conf нет, его необходимо создать:
`# echo "DEVICE partitions" > /etc/mdadm/mdadm.conf`
Далее необходимо добавить в него информацию о RAID-массиве:
`# mdadm --detail --scan --verbose | awk '/ARRAY/ {print}' >> /etc/mdadm/mdadm.conf`
## Замена диска в программном RAID-массиве
Имеем сбойный диск /dev/sdb в программном RAID-массиве. Необходимо его заменить. Прежде всего необходимо выполнить его диагностику:
`cat /proc/mdstat`
или
`mdadm --detail /dev/md0`
Если вместо [UU] видим [U_], то целостность одного из дисков нарушена - необходимо менять диск.
1. Помечаем разделы сбойного диска:
`mdadm --manage /dev/md0 --fail /dev/sdb1`
`mdadm --manage /dev/md1 --fail /dev/sdb2`
и т.д.
2. Отключаем разделы:
`mdadm --manage /dev/md0 --remove /dev/sdb1`
`mdadm --manage /dev/md1 --remove /dev/sdb2`
и т.д.
3. Выключаем сервер и меняем диск.
` shutdown -h now`
4. Копируем структуру разделов диска /dev/sda на новый диск /dev/sdb.
для DOS разделов:
`sfdisk -d /dev/sda | sfdisk --force /dev/sdb` -- ,
Для GPT разделов:
```
 yum -y install gdisk
 sgdisk --backup=table /dev/sda
 sgdisk --load-backup=table /dev/sdb
 sgdisk -G /dev/sdb
```
5. Проверяем, создание разделов:
` fdisk -l`
6. Последовательно добавляем разделы в RAID-массив:
`mdadm --manage /dev/md0 --add /dev/sdb1`
`mdadm --manage /dev/md1 --add /dev/sdb2`
и т.д.
Проверям процесс синхронизации командой:
` cat /proc/mdstat`
Если вышедший из строя диск /dev/sdb имел boot сектор, то его необходимо установить командой `grub-install /dev/sdb`.

## Другие функции mdadm
### Добавление нового диска
Добавить новый диск в массив можно с помощью ключей --add (-a) и --re-add:
`mdadm /dev/md0 --add /dev/sdb1`
`mdadm /dev/md0 -a    /dev/sdb1`
### Сборка сущестующего массива
Собрать существующий массив можно с помощью mdadm --assemble. Как дополнительный аргумент указывается, нужно ли выполнять сканирование устройств, и если нет, то какие устройства нужно собирать.
`mdadm --assemble /dev/md0 /dev/sdb1 /dev/sdc2 /dev/sdd`
`mdadm --assemble --scan`
### Расширение массива
Расширить массив можно с помощью ключа --grow (-G). Сначала добавляется диск, а потом массив расширяется:
`mdadm /dev/md0 --add /dev/sdb2`
Проверяем, что диск (раздел) добавился:
`mdadm --detail /dev/md0`
`cat /proc/mdstat`
Если раздел действительно добавился, мы можем расширить массив:
`mdadm -G /dev/md0 --raid-devices=4`
Опция --raid-devices указывает новое количество дисков используемое в массиве. Например, было 3 диска, а теперь расширяем до 4-х - указываем 4. Рекомендуется задать файл резервной копии на случай прерывания перестроения массива, например добавить:
`--backup-file=/var/backup`
При необходимости, можно регулировать скорость процесса расширения массива, указав нужное значение в файлах:
```
/proc/sys/dev/raid/speed_limit_min
/proc/sys/dev/raid/speed_limit_max
```
Убедимся, что массив расширился:
`cat /proc/mdstat`
Необходимо обновить конфигурационный файл с учётом сделанных изменений:
```
mdadm --detail --scan >> /etc/mdadm/mdadm.conf
vi /etc/mdadm/mdadm.conf
```
### Возобновление отложенной синхронизации
Отложенная синхронизация:
```
Personalities : [linear] [multipath] [raid0] [raid1] [raid6] [raid5] [raid4] [raid10]
md0 : active(auto-read-only) raid1 sda1[0] sdb1[1]
      78148096 blocks [2/2] [UU]
        resync=PENDING
```
Возобновить:
`echo idle > /sys/block/md0/md/sync_action`
Если Вы увидели «active (auto-read-only)» в файле /proc/mdstat, то возможно вы просто ничего не записывали в этот массив. К примеру, после монтирования раздела и любых изменений в примонтированном каталоге, статус автоматически меняется:
`md0 : active raid1 sdc[0] sdd[1]`
### Удаление массива
Для начала отмонтируйте и остановите массив:
`umount /dev/md0`
`mdadm -S /dev/md0`
Затем необходимо затереть superblock каждого из составляющих массива:
`mdadm --zero-superblock /dev/sda1`
`mdadm --zero-superblock /dev/sdb2`
Если действие выше не помогло, то затираем следующим образом:
`dd if=/dev/zero of=/dev/hde1 bs=512 count=1`
`dd if=/dev/zero of=/dev/hdf2 bs=512 count=1`

## Источники:
1. [Создание программного RAID массива в Ubuntu](http://help.ubuntu.ru/wiki/%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D0%B9_raid)
2. [Вебинар E=DC2 №2: Технология RAID и её применение](https://www.youtube.com/watch?v=bHIU4rZj2y8)
3. [Настройка программного RAID-1 на работающей системе Ubuntu 14](https://kamaok.org.ua/?p=1804)
4. [Замена жесткого диска в программном RAID1 в операционной системе Linux](http://www.sysadmin.in.ua/info/index/21/24/28)
5. [Замена диска в софт RAID массиве на ОС Linux](https://ru-tld.ru/h/help_system:servera:raid:zamenadiskasoftraidlin)
6. [mdadm](http://xgu.ru/wiki/mdadm)